{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from spektral.datasets import delaunay\n",
    "from spektral.layers import GraphAttention, GlobalAttentionPool, ARMAConv, GraphConv\n",
    "from spektral.utils import localpooling_filter\n",
    "from spektral.utils.logging import init_logging\n",
    "\n",
    "from spektral.utils import conversion\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "#adj, x, y = delaunay.generate_data(return_type='numpy', classes=[0, 5])\n",
    "\n",
    "\n",
    "#### import MUTAG\n",
    "from load_data import load_data\n",
    "graphs,labels = load_data(\"MUTAG\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### transform labels in the same format of y\n",
    "y = []\n",
    "for i in labels:\n",
    "    if(i==0):\n",
    "        y.append([1,0])\n",
    "    else:\n",
    "        y.append([0,1])\n",
    "        \n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from list of nx graphs to adj and x\n",
    "np_array = conversion.nx_to_numpy(graphs,nf_keys=['vec'])\n",
    "adj = np_array[0]\n",
    "x = np_array[1]\n",
    "len(adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = x.shape[-2]           # Number of nodes in the graphs\n",
    "F = x.shape[-1]           # Original feature dimensionality\n",
    "n_classes = y.shape[-1]   # Number of classes\n",
    "l2_reg = 5e-4             # Regularization rate for l2\n",
    "learning_rate = 1e-3      # Learning rate for Adam\n",
    "epochs = 200            # Number of training epochs\n",
    "batch_size = 32           # Batch size\n",
    "es_patience = 250          # Patience fot early stopping\n",
    "log_dir = init_logging()  # Create log directory and file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAECCAYAAAAYUakXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADTdJREFUeJzt3V+MHfV5xvHnwd5sE5sWW05XNnVKC9xEVWqSxUQtQq6jEuAGuEH1ReVKUc1FkELFRRA3cBMJVQGaiwrJFCuORKiQwMUXboPlRHJyEYs1QmBwW6zICMyyjmtRbKNs/OftxY7fHsiembM7Z87Mrr8fydo585tz5mXwPvrNzOs5jggBgCRd1XYBALqDQACQCAQAiUAAkAgEAIlAAJBaCQTbd9j+L9vHbD/cRg1lbB+3/abt121PdaCeXbZP2j7Ss26t7f223yl+rulYfY/ZPlEcw9dt39VifRtt/8z227bfsv2dYn0njmFJfSM/hh51H4LtFZL+W9JfS3pf0quStkXE2yMtpITt45ImI+JU27VIku3bJJ2V9KOI+LNi3T9KOh0RjxehuiYivtuh+h6TdDYivt9GTb1sr5e0PiJes321pMOS7pH0d+rAMSyp7z6N+Bi2MUPYLOlYRPwqIn4r6V8l3d1CHUtGRByUdPozq++WtLtY3q25v0Ct6FNfZ0TEdES8ViyfkXRU0rXqyDEsqW/k2giEayW91/P6fbX0H18iJL1i+7DtHW0X08dEREwXyx9KmmizmD4esP1GcUrR2ilNL9vXSbpJ0iF18Bh+pj5pxMeQi4rzuzUivirpTknfLqbEnRVz531d60F/WtL1kjZJmpb0RLvlSLZXS3pR0oMR8XHvWBeO4Tz1jfwYthEIJyRt7Hn9R8W6zoiIE8XPk5L2aO40p2tminPPy+egJ1uu51MiYiYiLkbEJUnPqOVjaHtMc79sz0XES8XqzhzD+epr4xi2EQivSrrR9p/Y/pykv5G0t4U65mV7VXFhR7ZXSbpd0pHyd7Vir6TtxfJ2SS+3WMvvuPyLVrhXLR5D25b0rKSjEfFkz1AnjmG/+to4hiO/yyBJxe2Tf5K0QtKuiPjeyIvow/afam5WIEkrJf247fpsPy9pi6R1kmYkPSrp3yS9IOlLkt6VdF9EtHJhr099WzQ31Q1JxyXd33O+Pur6bpX0c0lvSrpUrH5Ec+fprR/Dkvq2acTHsJVAANBNXFQEkAgEAIlAAJAIBACJQACQWg2EDrcFS6K+urpcX5drk9qrr+0ZQqf/p4j66upyfV2uTWqpvrYDAUCH1GpMsn2HpB9oruPwXyLi8bLtV6xeFSvXrs3XF8+e04rVq/L12Jny/V310blF1zqIS9es+tTr87NnNTa+emT7X6jzmtWYxtsuo68u19fl2qTh1/cbndNvY9ZV261c7A6KB538s3oedGJ7b9mDTlauXasNDz3Y9zM3HCwPpy/sOVQ6XtcnW29pdf9AUw7FgYG2q3PKwINOgGWmTiAshQedAFiAxi8q2t5he8r21MWz3ToHB/BpdQJhoAedRMTOiJiMiMneC4gAuqdOIHT6QScAFm7Rdxki4oLtByT9RP//oJO3yt4zdqb8TsIHt5XfFdmgZu8CcBcBV7pFB4IkRcQ+SfuGVAuAltGpCCARCAASgQAgEQgAEoEAIBEIAFKt244LddVH50rv9Vf1GVT1KWz8zc2l4+P//mrpOHClY4YAIBEIABKBACARCAASgQAgEQgAEoEAII20D6FK1fMIqvoMTn1lrHR8YvZrpeMrf3q4dLxpF7Z2uz4sf8wQACQCAUAiEAAkAgFAIhAAJAIBQCIQAKRO9SFUqXqeQVWfwczN5V+vvW68vM+hSt3nLdBngLYxQwCQCAQAiUAAkAgEAIlAAJAIBACJQACQRtqHcOmaVfpka//vXqh6HkKVqvv4VX0G792+onT8hn/45YJrApaSWoFg+7ikM5IuSroQEZPDKApAO4YxQ/iriDg1hM8B0DKuIQBIdQMhJL1i+7DtHcMoCEB76p4y3BoRJ2z/oaT9tv8zIg72blAExQ5J+tznr6m5OwBNqjVDiIgTxc+TkvZI2jzPNjsjYjIiJsfGV9fZHYCGLToQbK+yffXlZUm3SzoyrMIAjF6dU4YJSXtsX/6cH0fEf5S94aqPztXuNWhSVZ/Bsae+Xjq+8ZWLpeN1n5cANG3RgRARv5L050OsBUDLuO0IIBEIABKBACARCAASgQAgEQgA0pL6Xoa66vYBVPUZnPrKWOl41fdGdP17GS5sXdr1oxozBACJQACQCAQAiUAAkAgEAIlAAJAIBADpiupDqKuqj6Gqz2Dm5vHy96vd+/yzd5Z/bwXPc1j+mCEASAQCgEQgAEgEAoBEIABIBAKARCAASMuqD6Htf69f9flVfQZVfQrrxpvtE6DPAMwQACQCAUAiEAAkAgFAIhAAJAIBQCIQAKRl1YfQ9e8FqKqvqs/gvdtXlI5v+L1bSse/sOdQ6ThQOUOwvcv2SdtHetattb3f9jvFzzXNlglgFAY5ZfihpDs+s+5hSQci4kZJB4rXAJa4ykCIiIOSTn9m9d2SdhfLuyXdM+S6ALRgsRcVJyJiulj+UNLEkOoB0KLadxkiIiRFv3HbO2xP2Z46r9m6uwPQoMUGwozt9ZJU/DzZb8OI2BkRkxExOabyf80HoF2LDYS9krYXy9slvTyccgC0qbIPwfbzkrZIWmf7fUmPSnpc0gu2vyXpXUn3NVnklaLqeQRVfQYf3Oby94s+BZSrDISI2NZn6BtDrgVAy2hdBpAIBACJQACQCAQAiUAAkAgEAGlZPQ9huavqE6jqM6BPAVWYIQBIBAKARCAASAQCgEQgAEgEAoBEIABI9CGM0Oyd5d+7UPU8hCpN9yncsGfBJWGJYYYAIBEIABKBACARCAASgQAgEQgAEoEAINGHMEQXtn6tdLxun0FdVX0KVX0GJ777F6Xj1xy7uNCSPmXVizxvoW3MEAAkAgFAIhAAJAIBQCIQACQCAUAiEAAk+hCGaOVPD7ddQqOq+gwufOt/Ssf/4K5jwywHDaicIdjeZfuk7SM96x6zfcL268Wfu5otE8AoDHLK8ENJd8yz/qmI2FT82TfcsgC0oTIQIuKgpNMjqAVAy+pcVHzA9hvFKcWaoVUEoDWLDYSnJV0vaZOkaUlP9NvQ9g7bU7anzmt2kbsDMAqLCoSImImIixFxSdIzkjaXbLszIiYjYnJM44utE8AILCoQbK/veXmvpCP9tgWwdFT2Idh+XtIWSetsvy/pUUlbbG+SFJKOS7q/wRqxRFT1GfzvvhtqvR/NqwyEiNg2z+pnG6gFQMtoXQaQCAQAiUAAkAgEAIlAAJAIBACJ5yFgYHW/N6Gqz+AnH7xeOv7NDZtq7R/VmCEASAQCgEQgAEgEAoBEIABIBAKARCAASPQhoDOq+gzoU2geMwQAiUAAkAgEAIlAAJAIBACJQACQCAQAiT4ELBn0KTSPGQKARCAASAQCgEQgAEgEAoBEIABIBAKARB8Clg36FOqrnCHY3mj7Z7bftv2W7e8U69fa3m/7neLnmubLBdCkQU4ZLkh6KCK+LOnrkr5t+8uSHpZ0ICJulHSgeA1gCasMhIiYjojXiuUzko5KulbS3ZJ2F5vtlnRPU0UCGI0FXVS0fZ2kmyQdkjQREdPF0IeSJoZaGYCRGzgQbK+W9KKkByPi496xiAhJ0ed9O2xP2Z46r9laxQJo1kCBYHtMc2HwXES8VKyesb2+GF8v6eR8742InRExGRGTYxofRs0AGjLIXQZLelbS0Yh4smdor6TtxfJ2SS8PvzwAo+S52X7JBvatkn4u6U1Jl4rVj2juOsILkr4k6V1J90XE6bLP+n2vjVv8jbo1A41Yzn0Kh+KAPo7TrtqusjEpIn4hqd8H8dsNLCO0LgNIBAKARCAASAQCgEQgAEgEAoDE8xCAAs9TYIYAoAeBACARCAASgQAgEQgAEoEAIBEIABJ9CMCAroQ+BWYIABKBACARCAASgQAgEQgAEoEAIBEIABJ9CMCQLIc+BWYIABKBACARCAASgQAgEQgAEoEAIBEIAFJlH4LtjZJ+JGlCUkjaGRE/sP2YpL+X9Oti00ciYl9ThQJL3VLoUxikMemCpIci4jXbV0s6bHt/MfZURHy/ufIAjFJlIETEtKTpYvmM7aOSrm26MACjt6BrCLavk3STpEPFqgdsv2F7l+01Q64NwIgNHAi2V0t6UdKDEfGxpKclXS9pk+ZmEE/0ed8O21O2p85rdgglA2jKQIFge0xzYfBcRLwkSRExExEXI+KSpGckbZ7vvRGxMyImI2JyTOPDqhtAAyoDwbYlPSvpaEQ82bN+fc9m90o6MvzyAIzSIHcZ/lLS30p60/bl+yKPSNpme5PmbkUel3R/IxUCGJlB7jL8QpLnGaLnABiiLvQp0KkIIBEIABKBACARCAASgQAgEQgAEoEAIPG9DMASUadPYfM3PxloH8wQACQCAUAiEAAkAgFAIhAAJAIBQCIQACRHxOh2Zv9a0rs9q9ZJOjWyAhaO+urpcn1drk0afn1/HBFfrNpopIHwOzu3pyJisrUCKlBfPV2ur8u1Se3VxykDgEQgAEhtB8LOlvdfhfrq6XJ9Xa5Naqm+Vq8hAOiWtmcIADqEQACQCAQAiUAAkAgEAOn/AC7B401tj4tJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "fltr = localpooling_filter(adj.copy())\n",
    "len(fltr[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(fltr[10])\n",
    "\n",
    "plt.show()\n",
    "plt.matshow(adj[10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "fltr_train, fltr_test, \\\n",
    "x_train, x_test,       \\\n",
    "y_train, y_test = train_test_split(adj, x, y, test_size=0.1)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(N, F)) # shape 28 * 3     # featrues\n",
    "filter_in = Input((N, N))  # shape 28 * 28    # matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 28, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arma_conv_29 (ARMAConv)         (None, 28, 16)       112         input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "node_attention_pool_16 (NodeAtt (None, 16)           16          arma_conv_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 2)            34          node_attention_pool_16[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 162\n",
      "Trainable params: 162\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from spektral.layers import ARMAConv\n",
    "from spektral.layers import NodeAttentionPool\n",
    "\n",
    "# input  [(28*3),(28x28)]\n",
    "# output [28x32]\n",
    "channels = 16\n",
    "arma1 = ARMAConv(channels,ARMA_D=1, activation='relu')([X_in, filter_in])\n",
    "pool1 = NodeAttentionPool()(arma1)\n",
    "\n",
    "\n",
    "output = Dense(n_classes, activation='softmax')(pool1)\n",
    "\n",
    "# Build model\n",
    "model_1= Model(inputs=[X_in, filter_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_1.summary()\n",
    "plot_model(model_1, to_file='Spektral_model_tmp_1.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model_1, to_file='Spektral_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 28, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_2 (GraphConv)        (None, 28, 32)       128         input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_1 (GraphAttenti (None, 28, 32)       1120        graph_conv_2[0][0]               \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_2 (GraphAttenti (None, 28, 32)       1120        graph_attention_1[0][0]          \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_attention_pool_1 (Global (None, 128)          8448        graph_attention_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         global_attention_pool_1[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 11,074\n",
      "Trainable params: 11,074\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 152 samples, validate on 17 samples\n",
      "Epoch 1/200\n",
      "152/152 [==============================] - 5s 31ms/step - loss: 0.8812 - acc: 0.5461 - val_loss: 0.8803 - val_acc: 0.6471\n",
      "Epoch 2/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.7713 - acc: 0.6579 - val_loss: 0.6117 - val_acc: 0.6471\n",
      "Epoch 3/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.6608 - acc: 0.6316 - val_loss: 0.6645 - val_acc: 0.8235\n",
      "Epoch 4/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.5902 - acc: 0.7303 - val_loss: 0.5777 - val_acc: 0.6471\n",
      "Epoch 5/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.5568 - acc: 0.6711 - val_loss: 0.5192 - val_acc: 0.7059\n",
      "Epoch 6/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.5113 - acc: 0.7961 - val_loss: 0.5294 - val_acc: 0.8824\n",
      "Epoch 7/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.4728 - acc: 0.8289 - val_loss: 0.4613 - val_acc: 0.8235\n",
      "Epoch 8/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.4582 - acc: 0.7697 - val_loss: 0.4346 - val_acc: 0.8235\n",
      "Epoch 9/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.4136 - acc: 0.8224 - val_loss: 0.4519 - val_acc: 0.8824\n",
      "Epoch 10/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.4294 - acc: 0.8553 - val_loss: 0.4038 - val_acc: 0.8824\n",
      "Epoch 11/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3973 - acc: 0.8355 - val_loss: 0.3719 - val_acc: 0.8235\n",
      "Epoch 12/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3938 - acc: 0.8487 - val_loss: 0.3773 - val_acc: 0.8824\n",
      "Epoch 13/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3838 - acc: 0.8684 - val_loss: 0.3645 - val_acc: 0.8824\n",
      "Epoch 14/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.8684 - val_loss: 0.3390 - val_acc: 0.8824\n",
      "Epoch 15/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3809 - acc: 0.8553 - val_loss: 0.3383 - val_acc: 0.8824\n",
      "Epoch 16/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3808 - acc: 0.8618 - val_loss: 0.3299 - val_acc: 0.8824\n",
      "Epoch 17/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3753 - acc: 0.8684 - val_loss: 0.3281 - val_acc: 0.8824\n",
      "Epoch 18/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3707 - acc: 0.8618 - val_loss: 0.3446 - val_acc: 0.8824\n",
      "Epoch 19/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3733 - acc: 0.8750 - val_loss: 0.3224 - val_acc: 0.8824\n",
      "Epoch 20/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8684 - val_loss: 0.3283 - val_acc: 0.8824\n",
      "Epoch 21/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3709 - acc: 0.8618 - val_loss: 0.3324 - val_acc: 0.8824\n",
      "Epoch 22/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3702 - acc: 0.8618 - val_loss: 0.3225 - val_acc: 0.8824\n",
      "Epoch 23/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3693 - acc: 0.8618 - val_loss: 0.3336 - val_acc: 0.8824\n",
      "Epoch 24/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3750 - acc: 0.8684 - val_loss: 0.3216 - val_acc: 0.8824\n",
      "Epoch 25/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3905 - acc: 0.8487 - val_loss: 0.3439 - val_acc: 0.8824\n",
      "Epoch 26/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3737 - acc: 0.8487 - val_loss: 0.3102 - val_acc: 0.8824\n",
      "Epoch 27/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3784 - acc: 0.8618 - val_loss: 0.3483 - val_acc: 0.8824\n",
      "Epoch 28/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3718 - acc: 0.8684 - val_loss: 0.3225 - val_acc: 0.8824\n",
      "Epoch 29/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3760 - acc: 0.8618 - val_loss: 0.3202 - val_acc: 0.8824\n",
      "Epoch 30/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3675 - acc: 0.8684 - val_loss: 0.3501 - val_acc: 0.8824\n",
      "Epoch 31/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3852 - acc: 0.8618 - val_loss: 0.3149 - val_acc: 0.8824\n",
      "Epoch 32/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3786 - acc: 0.8816 - val_loss: 0.3536 - val_acc: 0.8824\n",
      "Epoch 33/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3654 - acc: 0.8750 - val_loss: 0.3070 - val_acc: 0.8824\n",
      "Epoch 34/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3740 - acc: 0.8553 - val_loss: 0.3293 - val_acc: 0.8824\n",
      "Epoch 35/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3673 - acc: 0.8684 - val_loss: 0.3165 - val_acc: 0.8824\n",
      "Epoch 36/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3799 - acc: 0.8684 - val_loss: 0.3419 - val_acc: 0.8824\n",
      "Epoch 37/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3806 - acc: 0.8553 - val_loss: 0.3059 - val_acc: 0.8824\n",
      "Epoch 38/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3649 - acc: 0.8553 - val_loss: 0.3770 - val_acc: 0.8824\n",
      "Epoch 39/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3747 - acc: 0.8684 - val_loss: 0.3194 - val_acc: 0.8824\n",
      "Epoch 40/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3684 - acc: 0.8487 - val_loss: 0.3065 - val_acc: 0.8824\n",
      "Epoch 41/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3946 - acc: 0.8553 - val_loss: 0.3374 - val_acc: 0.8824\n",
      "Epoch 42/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3561 - acc: 0.8750 - val_loss: 0.3068 - val_acc: 0.8824\n",
      "Epoch 43/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3769 - acc: 0.8421 - val_loss: 0.3210 - val_acc: 0.8824\n",
      "Epoch 44/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3635 - acc: 0.8684 - val_loss: 0.3364 - val_acc: 0.8824\n",
      "Epoch 45/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3608 - acc: 0.8684 - val_loss: 0.3165 - val_acc: 0.8824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "152/152 [==============================] - 1s 4ms/step - loss: 0.3670 - acc: 0.8684 - val_loss: 0.3230 - val_acc: 0.8824\n",
      "Epoch 47/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3615 - acc: 0.8684 - val_loss: 0.3121 - val_acc: 0.8824\n",
      "Epoch 48/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3748 - acc: 0.8553 - val_loss: 0.3110 - val_acc: 0.8824\n",
      "Epoch 49/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3618 - acc: 0.8553 - val_loss: 0.3448 - val_acc: 0.8824\n",
      "Epoch 50/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3646 - acc: 0.8816 - val_loss: 0.3120 - val_acc: 0.8824\n",
      "Epoch 51/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3640 - acc: 0.8618 - val_loss: 0.3135 - val_acc: 0.8824\n",
      "Epoch 52/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3657 - acc: 0.8618 - val_loss: 0.3275 - val_acc: 0.8824\n",
      "Epoch 53/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3579 - acc: 0.8684 - val_loss: 0.3099 - val_acc: 0.8824\n",
      "Epoch 54/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3671 - acc: 0.8553 - val_loss: 0.3099 - val_acc: 0.8824\n",
      "Epoch 55/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3690 - acc: 0.8750 - val_loss: 0.3630 - val_acc: 0.8824\n",
      "Epoch 56/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3598 - acc: 0.8684 - val_loss: 0.3038 - val_acc: 0.8824\n",
      "Epoch 57/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3677 - acc: 0.8421 - val_loss: 0.3175 - val_acc: 0.8824\n",
      "Epoch 58/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3800 - acc: 0.8487 - val_loss: 0.3437 - val_acc: 0.8824\n",
      "Epoch 59/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.4054 - acc: 0.8289 - val_loss: 0.3040 - val_acc: 0.8824\n",
      "Epoch 60/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3802 - acc: 0.8487 - val_loss: 0.3893 - val_acc: 0.8824\n",
      "Epoch 61/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3594 - acc: 0.8816 - val_loss: 0.3034 - val_acc: 0.8824\n",
      "Epoch 62/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3936 - acc: 0.8355 - val_loss: 0.3037 - val_acc: 0.8824\n",
      "Epoch 63/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3679 - acc: 0.8618 - val_loss: 0.3687 - val_acc: 0.8824\n",
      "Epoch 64/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3627 - acc: 0.8684 - val_loss: 0.3038 - val_acc: 0.8824\n",
      "Epoch 65/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3638 - acc: 0.8618 - val_loss: 0.3156 - val_acc: 0.8824\n",
      "Epoch 66/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3577 - acc: 0.8684 - val_loss: 0.3217 - val_acc: 0.8824\n",
      "Epoch 67/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3587 - acc: 0.8684 - val_loss: 0.3242 - val_acc: 0.8824\n",
      "Epoch 68/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3563 - acc: 0.8684 - val_loss: 0.3058 - val_acc: 0.8824\n",
      "Epoch 69/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3572 - acc: 0.8618 - val_loss: 0.3162 - val_acc: 0.8824\n",
      "Epoch 70/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3597 - acc: 0.8618 - val_loss: 0.3184 - val_acc: 0.8824\n",
      "Epoch 71/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3602 - acc: 0.8816 - val_loss: 0.3195 - val_acc: 0.8824\n",
      "Epoch 72/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3550 - acc: 0.8684 - val_loss: 0.3031 - val_acc: 0.8824\n",
      "Epoch 73/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3586 - acc: 0.8684 - val_loss: 0.3207 - val_acc: 0.8824\n",
      "Epoch 74/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3580 - acc: 0.8684 - val_loss: 0.3191 - val_acc: 0.8824\n",
      "Epoch 75/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3565 - acc: 0.8618 - val_loss: 0.3078 - val_acc: 0.8824\n",
      "Epoch 76/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3592 - acc: 0.8618 - val_loss: 0.3344 - val_acc: 0.8824\n",
      "Epoch 77/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3599 - acc: 0.8618 - val_loss: 0.3060 - val_acc: 0.8824\n",
      "Epoch 78/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3637 - acc: 0.8618 - val_loss: 0.3158 - val_acc: 0.8824\n",
      "Epoch 79/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3547 - acc: 0.8684 - val_loss: 0.3041 - val_acc: 0.8824\n",
      "Epoch 80/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3616 - acc: 0.8684 - val_loss: 0.3319 - val_acc: 0.8824\n",
      "Epoch 81/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3577 - acc: 0.8618 - val_loss: 0.3205 - val_acc: 0.8824\n",
      "Epoch 82/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3535 - acc: 0.8618 - val_loss: 0.3027 - val_acc: 0.8824\n",
      "Epoch 83/200\n",
      "152/152 [==============================] - 1s 3ms/step - loss: 0.3615 - acc: 0.8618 - val_loss: 0.3277 - val_acc: 0.8824\n",
      "Epoch 84/200\n",
      "152/152 [==============================] - 1s 3ms/step - loss: 0.3591 - acc: 0.8750 - val_loss: 0.3197 - val_acc: 0.8824\n",
      "Epoch 85/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3582 - acc: 0.8618 - val_loss: 0.3102 - val_acc: 0.8824\n",
      "Epoch 86/200\n",
      "152/152 [==============================] - 1s 4ms/step - loss: 0.3583 - acc: 0.8618 - val_loss: 0.3050 - val_acc: 0.8824\n",
      "Epoch 87/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3552 - acc: 0.8618 - val_loss: 0.3102 - val_acc: 0.8824\n",
      "Epoch 88/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3574 - acc: 0.8684 - val_loss: 0.3274 - val_acc: 0.8824\n",
      "Epoch 89/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3546 - acc: 0.8684 - val_loss: 0.3222 - val_acc: 0.8824\n",
      "Epoch 90/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3588 - acc: 0.8684 - val_loss: 0.3218 - val_acc: 0.8824\n",
      "Epoch 91/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3543 - acc: 0.8684 - val_loss: 0.3110 - val_acc: 0.8824\n",
      "Epoch 92/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3537 - acc: 0.8553 - val_loss: 0.3061 - val_acc: 0.8824\n",
      "Epoch 93/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3755 - acc: 0.8553 - val_loss: 0.3145 - val_acc: 0.8824\n",
      "Epoch 94/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3986 - acc: 0.8421 - val_loss: 0.2972 - val_acc: 0.8824\n",
      "Epoch 95/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3745 - acc: 0.8553 - val_loss: 0.3778 - val_acc: 0.8824\n",
      "Epoch 96/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3620 - acc: 0.8618 - val_loss: 0.2976 - val_acc: 0.8824\n",
      "Epoch 97/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3678 - acc: 0.8553 - val_loss: 0.3285 - val_acc: 0.8824\n",
      "Epoch 98/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3553 - acc: 0.8618 - val_loss: 0.3279 - val_acc: 0.8824\n",
      "Epoch 99/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3555 - acc: 0.8684 - val_loss: 0.3033 - val_acc: 0.8824\n",
      "Epoch 100/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3520 - acc: 0.8684 - val_loss: 0.3272 - val_acc: 0.8824\n",
      "Epoch 101/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3583 - acc: 0.8553 - val_loss: 0.3170 - val_acc: 0.8824\n",
      "Epoch 102/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3553 - acc: 0.8618 - val_loss: 0.3107 - val_acc: 0.8824\n",
      "Epoch 103/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3527 - acc: 0.8684 - val_loss: 0.3046 - val_acc: 0.8824\n",
      "Epoch 104/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3538 - acc: 0.8684 - val_loss: 0.3141 - val_acc: 0.8824\n",
      "Epoch 105/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3531 - acc: 0.8684 - val_loss: 0.3094 - val_acc: 0.8824\n",
      "Epoch 106/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3626 - acc: 0.8553 - val_loss: 0.3256 - val_acc: 0.8824\n",
      "Epoch 107/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3758 - acc: 0.8421 - val_loss: 0.3328 - val_acc: 0.8824\n",
      "Epoch 108/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3617 - acc: 0.8618 - val_loss: 0.2967 - val_acc: 0.8824\n",
      "Epoch 109/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3546 - acc: 0.8618 - val_loss: 0.3407 - val_acc: 0.8824\n",
      "Epoch 110/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.8421 - val_loss: 0.3134 - val_acc: 0.8824\n",
      "Epoch 111/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3540 - acc: 0.8618 - val_loss: 0.2988 - val_acc: 0.8824\n",
      "Epoch 112/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3557 - acc: 0.8684 - val_loss: 0.3288 - val_acc: 0.8824\n",
      "Epoch 113/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3503 - acc: 0.8618 - val_loss: 0.3037 - val_acc: 0.8824\n",
      "Epoch 114/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3554 - acc: 0.8618 - val_loss: 0.3041 - val_acc: 0.8824\n",
      "Epoch 115/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3567 - acc: 0.8618 - val_loss: 0.3420 - val_acc: 0.8824\n",
      "Epoch 116/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3579 - acc: 0.8553 - val_loss: 0.2965 - val_acc: 0.8824\n",
      "Epoch 117/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3643 - acc: 0.8553 - val_loss: 0.3313 - val_acc: 0.8824\n",
      "Epoch 118/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3570 - acc: 0.8618 - val_loss: 0.3046 - val_acc: 0.8824\n",
      "Epoch 119/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3537 - acc: 0.8750 - val_loss: 0.3168 - val_acc: 0.8824\n",
      "Epoch 120/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3503 - acc: 0.8684 - val_loss: 0.3194 - val_acc: 0.8824\n",
      "Epoch 121/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3505 - acc: 0.8684 - val_loss: 0.3157 - val_acc: 0.8824\n",
      "Epoch 122/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3565 - acc: 0.8618 - val_loss: 0.3179 - val_acc: 0.8824\n",
      "Epoch 123/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3661 - acc: 0.8487 - val_loss: 0.2962 - val_acc: 0.8824\n",
      "Epoch 124/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3527 - acc: 0.8684 - val_loss: 0.3484 - val_acc: 0.8824\n",
      "Epoch 125/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3659 - acc: 0.8618 - val_loss: 0.3059 - val_acc: 0.8824\n",
      "Epoch 126/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3579 - acc: 0.8618 - val_loss: 0.2979 - val_acc: 0.8824\n",
      "Epoch 127/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3580 - acc: 0.8684 - val_loss: 0.3417 - val_acc: 0.8824\n",
      "Epoch 128/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3501 - acc: 0.8618 - val_loss: 0.2973 - val_acc: 0.8824\n",
      "Epoch 129/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3645 - acc: 0.8618 - val_loss: 0.3051 - val_acc: 0.8824\n",
      "Epoch 130/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3511 - acc: 0.8618 - val_loss: 0.3146 - val_acc: 0.8824\n",
      "Epoch 131/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3555 - acc: 0.8553 - val_loss: 0.3186 - val_acc: 0.8824\n",
      "Epoch 132/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3467 - acc: 0.8684 - val_loss: 0.2958 - val_acc: 0.8824\n",
      "Epoch 133/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3514 - acc: 0.8618 - val_loss: 0.3211 - val_acc: 0.8824\n",
      "Epoch 134/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3561 - acc: 0.8684 - val_loss: 0.3090 - val_acc: 0.8824\n",
      "Epoch 135/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3635 - acc: 0.8553 - val_loss: 0.3033 - val_acc: 0.8824\n",
      "Epoch 136/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3552 - acc: 0.8684 - val_loss: 0.3103 - val_acc: 0.8824\n",
      "Epoch 137/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3495 - acc: 0.8750 - val_loss: 0.3490 - val_acc: 0.8824\n",
      "Epoch 138/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3548 - acc: 0.8816 - val_loss: 0.2967 - val_acc: 0.8824\n",
      "Epoch 139/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3636 - acc: 0.8684 - val_loss: 0.3152 - val_acc: 0.8824\n",
      "Epoch 140/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3525 - acc: 0.8684 - val_loss: 0.2986 - val_acc: 0.8824\n",
      "Epoch 141/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3485 - acc: 0.8684 - val_loss: 0.3233 - val_acc: 0.8824\n",
      "Epoch 142/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3499 - acc: 0.8618 - val_loss: 0.3204 - val_acc: 0.8824\n",
      "Epoch 143/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3481 - acc: 0.8684 - val_loss: 0.3066 - val_acc: 0.8824\n",
      "Epoch 144/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3486 - acc: 0.8684 - val_loss: 0.3032 - val_acc: 0.8824\n",
      "Epoch 145/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3504 - acc: 0.8553 - val_loss: 0.3176 - val_acc: 0.8824\n",
      "Epoch 146/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3558 - acc: 0.8618 - val_loss: 0.3064 - val_acc: 0.8824\n",
      "Epoch 147/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3530 - acc: 0.8553 - val_loss: 0.3189 - val_acc: 0.8824\n",
      "Epoch 148/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3633 - acc: 0.8487 - val_loss: 0.3148 - val_acc: 0.8824\n",
      "Epoch 149/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3420 - acc: 0.8684 - val_loss: 0.2929 - val_acc: 0.8824\n",
      "Epoch 150/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3574 - acc: 0.8618 - val_loss: 0.3037 - val_acc: 0.8824\n",
      "Epoch 151/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3507 - acc: 0.8684 - val_loss: 0.3322 - val_acc: 0.8824\n",
      "Epoch 152/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3667 - acc: 0.8684 - val_loss: 0.2997 - val_acc: 0.8824\n",
      "Epoch 153/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3516 - acc: 0.8553 - val_loss: 0.2965 - val_acc: 0.8824\n",
      "Epoch 154/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3559 - acc: 0.8553 - val_loss: 0.3179 - val_acc: 0.8824\n",
      "Epoch 155/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3491 - acc: 0.8618 - val_loss: 0.2972 - val_acc: 0.8824\n",
      "Epoch 156/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3630 - acc: 0.8618 - val_loss: 0.3052 - val_acc: 0.8824\n",
      "Epoch 157/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3529 - acc: 0.8816 - val_loss: 0.3492 - val_acc: 0.8824\n",
      "Epoch 158/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3490 - acc: 0.8553 - val_loss: 0.2961 - val_acc: 0.8824\n",
      "Epoch 159/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3526 - acc: 0.8553 - val_loss: 0.3123 - val_acc: 0.8824\n",
      "Epoch 160/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3471 - acc: 0.8684 - val_loss: 0.3097 - val_acc: 0.8824\n",
      "Epoch 161/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3484 - acc: 0.8684 - val_loss: 0.3102 - val_acc: 0.8824\n",
      "Epoch 162/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3515 - acc: 0.8750 - val_loss: 0.3039 - val_acc: 0.8824\n",
      "Epoch 163/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3458 - acc: 0.8618 - val_loss: 0.3344 - val_acc: 0.8824\n",
      "Epoch 164/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3554 - acc: 0.8553 - val_loss: 0.2973 - val_acc: 0.8824\n",
      "Epoch 165/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3487 - acc: 0.8684 - val_loss: 0.3250 - val_acc: 0.8824\n",
      "Epoch 166/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3521 - acc: 0.8553 - val_loss: 0.2949 - val_acc: 0.8824\n",
      "Epoch 167/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3533 - acc: 0.8684 - val_loss: 0.3306 - val_acc: 0.8824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3532 - acc: 0.8684 - val_loss: 0.2968 - val_acc: 0.8824\n",
      "Epoch 169/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3467 - acc: 0.8618 - val_loss: 0.3218 - val_acc: 0.8824\n",
      "Epoch 170/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3795 - acc: 0.8421 - val_loss: 0.3020 - val_acc: 0.8824\n",
      "Epoch 171/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3910 - acc: 0.8289 - val_loss: 0.2914 - val_acc: 0.8824\n",
      "Epoch 172/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3631 - acc: 0.8421 - val_loss: 0.3822 - val_acc: 0.8824\n",
      "Epoch 173/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3596 - acc: 0.8421 - val_loss: 0.2935 - val_acc: 0.8824\n",
      "Epoch 174/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3645 - acc: 0.8618 - val_loss: 0.3299 - val_acc: 0.8824\n",
      "Epoch 175/200\n",
      "152/152 [==============================] - 0s 987us/step - loss: 0.3510 - acc: 0.8816 - val_loss: 0.3005 - val_acc: 0.8824\n",
      "Epoch 176/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3522 - acc: 0.8750 - val_loss: 0.3167 - val_acc: 0.8824\n",
      "Epoch 177/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3501 - acc: 0.8684 - val_loss: 0.3005 - val_acc: 0.8824\n",
      "Epoch 178/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3469 - acc: 0.8553 - val_loss: 0.3170 - val_acc: 0.8824\n",
      "Epoch 179/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3830 - acc: 0.8553 - val_loss: 0.3117 - val_acc: 0.8824\n",
      "Epoch 180/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3509 - acc: 0.8684 - val_loss: 0.2936 - val_acc: 0.8824\n",
      "Epoch 181/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3590 - acc: 0.8553 - val_loss: 0.3469 - val_acc: 0.8824\n",
      "Epoch 182/200\n",
      "152/152 [==============================] - 0s 3ms/step - loss: 0.3582 - acc: 0.8947 - val_loss: 0.3051 - val_acc: 0.8824\n",
      "Epoch 183/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3616 - acc: 0.8553 - val_loss: 0.3017 - val_acc: 0.8824\n",
      "Epoch 184/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3496 - acc: 0.8684 - val_loss: 0.3299 - val_acc: 0.8824\n",
      "Epoch 185/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3497 - acc: 0.8618 - val_loss: 0.2996 - val_acc: 0.8824\n",
      "Epoch 186/200\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 0.3459 - acc: 0.8684 - val_loss: 0.3058 - val_acc: 0.8824\n",
      "Epoch 187/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3505 - acc: 0.8684 - val_loss: 0.3109 - val_acc: 0.8824\n",
      "Epoch 188/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3448 - acc: 0.8684 - val_loss: 0.2990 - val_acc: 0.8824\n",
      "Epoch 189/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3458 - acc: 0.8684 - val_loss: 0.3047 - val_acc: 0.8824\n",
      "Epoch 190/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3452 - acc: 0.8618 - val_loss: 0.3181 - val_acc: 0.8824\n",
      "Epoch 191/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3460 - acc: 0.8618 - val_loss: 0.2931 - val_acc: 0.8824\n",
      "Epoch 192/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3597 - acc: 0.8487 - val_loss: 0.3297 - val_acc: 0.8824\n",
      "Epoch 193/200\n",
      "152/152 [==============================] - 0s 968us/step - loss: 0.3565 - acc: 0.8618 - val_loss: 0.2928 - val_acc: 0.8824\n",
      "Epoch 194/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3491 - acc: 0.8553 - val_loss: 0.3304 - val_acc: 0.8824\n",
      "Epoch 195/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3533 - acc: 0.8684 - val_loss: 0.2973 - val_acc: 0.8824\n",
      "Epoch 196/200\n",
      "152/152 [==============================] - 0s 994us/step - loss: 0.3466 - acc: 0.8684 - val_loss: 0.3119 - val_acc: 0.8824\n",
      "Epoch 197/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3489 - acc: 0.8750 - val_loss: 0.3019 - val_acc: 0.8824\n",
      "Epoch 198/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3478 - acc: 0.8684 - val_loss: 0.2940 - val_acc: 0.8824\n",
      "Epoch 199/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3450 - acc: 0.8618 - val_loss: 0.3367 - val_acc: 0.8824\n",
      "Epoch 200/200\n",
      "152/152 [==============================] - 0s 1ms/step - loss: 0.3516 - acc: 0.8684 - val_loss: 0.3062 - val_acc: 0.8824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a76309400>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "fltr = localpooling_filter(adj.copy())\n",
    "\n",
    "# Train/test split\n",
    "fltr_train, fltr_test, \\\n",
    "x_train, x_test,       \\\n",
    "y_train, y_test = train_test_split(fltr, x, y, test_size=0.1)\n",
    "\n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(N, F)) # shape 28 * 3     # featrues\n",
    "filter_in = Input((N, N))  # shape 28 * 28    # matrix\n",
    "\n",
    "# input  [(28*3),(28x28)]\n",
    "# output [28x32]\n",
    "gc1 = GraphConv(32, activation='relu', kernel_regularizer=l2(l2_reg))([X_in, filter_in]) \n",
    "\n",
    "# input  [(28*32),(28x28)]\n",
    "# output [28x32]\n",
    "gc2 = GraphAttention(32, activation='relu', kernel_regularizer=l2(l2_reg))([gc1, filter_in])\n",
    "\n",
    "\n",
    "gc3 = GraphAttention(32, activation='relu', kernel_regularizer=l2(l2_reg))([gc2, filter_in])\n",
    "# input  (28*32)\n",
    "# output [128]\n",
    "pool = GlobalAttentionPool(128)(gc3)\n",
    "\n",
    "# input  [128]\n",
    "# output [2]\n",
    "output = Dense(n_classes, activation='softmax')(pool)\n",
    "\n",
    "# Build model\n",
    "model_1= Model(inputs=[X_in, filter_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_1.summary()\n",
    "\n",
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_acc', patience=es_patience)\n",
    "\n",
    "# Train model\n",
    "model_1.fit([x_train, fltr_train],\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1,\n",
    "          epochs=epochs,\n",
    "          callbacks=[es_callback])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model.\n",
      "19/19 [==============================] - 0s 657us/step\n",
      "Done.\n",
      "Test loss: 0.3925187885761261\n",
      "Test accuracy: 0.8421052694320679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nEvaluating model.\\n19/19 [==============================] - 0s 938us/step\\nDone.\\nTest loss: 0.4744948446750641\\nTest accuracy: 0.8421052694320679\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model_1.evaluate([x_test, fltr_test],\n",
    "                              y_test,\n",
    "                              batch_size=batch_size)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "'Test accuracy: {}'.format(*eval_results))\n",
    "\n",
    "\n",
    "'''\n",
    "Evaluating model.\n",
    "19/19 [==============================] - 0s 938us/step\n",
    "Done.\n",
    "Test loss: 0.4744948446750641\n",
    "Test accuracy: 0.8421052694320679\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "costruisci il secondo modello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build model\n",
    "model_1 = Model(inputs=[X_in, filter_in], outputs=pool)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.83060813e-01, -1.45765102e+00, -4.62713718e-01,\n",
       "         1.20945084e+00, -3.65347058e-01,  2.40273938e-01,\n",
       "         1.13910234e+00,  1.06065333e+00, -6.60582781e-02,\n",
       "        -3.16698328e-02, -6.31348848e-01,  2.53560829e+00,\n",
       "         2.78621650e+00,  1.03471386e+00, -1.94535553e+00,\n",
       "        -1.67841345e-01,  7.14936018e-01, -1.68904710e+00,\n",
       "        -1.13946572e-01, -1.12724340e+00,  7.18472004e-01,\n",
       "         1.75077677e+00, -7.75716007e-01, -5.20794392e-01,\n",
       "         6.75320566e-01,  4.14868481e-02, -4.42042887e-01,\n",
       "         4.53033745e-01,  1.11571479e+00,  1.54021132e+00,\n",
       "         1.16421652e+00,  8.58242631e-01, -1.06086957e+00,\n",
       "         5.28394401e-01, -2.02828121e+00,  2.13820376e-02,\n",
       "         2.40367144e-01, -2.61864662e-01, -8.57379138e-01,\n",
       "        -4.98907745e-01, -2.34041825e-01, -8.53062391e-01,\n",
       "        -1.40194580e-01, -3.71747762e-01,  2.96175122e-01,\n",
       "         1.33437419e+00, -5.86518407e-01,  6.06410682e-01,\n",
       "         4.00160253e-01,  3.53689305e-04, -1.02644992e+00,\n",
       "         1.41695523e+00, -1.20202577e+00, -5.21584868e-01,\n",
       "        -2.24142599e+00,  2.59504819e+00,  1.12347257e+00,\n",
       "         6.11419201e-01,  7.64817417e-01,  5.90087950e-01,\n",
       "        -6.16403699e-01,  3.11157793e-01,  1.95343924e+00,\n",
       "        -2.68329954e+00,  4.17523563e-01, -1.52153027e+00,\n",
       "        -1.93298924e+00,  1.65792301e-01, -6.19363964e-01,\n",
       "         1.19720161e+00, -7.44679689e-01, -1.15590179e+00,\n",
       "        -9.14384723e-01, -4.92522120e-01, -1.45124030e+00,\n",
       "        -1.12310886e+00, -3.72387737e-01,  7.93762028e-01,\n",
       "         5.08373559e-01,  3.29941094e-01, -6.80106580e-01,\n",
       "         1.32755506e+00,  1.30450225e+00, -2.31452346e-01,\n",
       "        -1.89498097e-01,  5.26588380e-01,  1.27637535e-01,\n",
       "        -1.03882289e+00,  1.11492205e+00,  6.42787158e-01,\n",
       "        -7.95971416e-03, -2.65848112e+00, -1.68236208e+00,\n",
       "        -8.52018669e-02,  3.48353460e-02, -2.08631587e+00,\n",
       "         8.41902077e-01, -2.28813455e-01, -6.88764215e-01,\n",
       "        -4.57472980e-01, -1.69673336e+00,  6.67205513e-01,\n",
       "        -1.24873948e+00,  4.87949371e-01, -2.57003164e+00,\n",
       "        -7.63357401e-01, -3.98765802e-01, -7.46917427e-01,\n",
       "         3.60575795e-01,  3.24025340e-02,  3.99419397e-01,\n",
       "        -6.41287446e-01,  2.11172295e+00,  1.51841855e+00,\n",
       "        -2.30227280e+00,  6.94041550e-01, -6.57814026e-01,\n",
       "         9.10382092e-01,  1.41546941e+00,  6.24562316e-02,\n",
       "        -1.84227288e-01, -3.20964622e+00,  9.81547683e-02,\n",
       "         1.19264185e+00, -1.36089766e+00, -1.03769824e-01,\n",
       "        -1.32301223e+00,  1.47567320e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = model_1.predict([x_test, fltr_test],batch_size=batch_size)\n",
    "\n",
    "predictions[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_test_tmp = []\n",
    "\n",
    "for i in y_test:\n",
    "    if (i[0] == 0):\n",
    "        y_test_tmp.append(1)\n",
    "    else:\n",
    "        y_test_tmp.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/sklearn/model_selection/_split.py:652: Warning: The least populated class in y has only 8 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from embedder import Embedder\n",
    "from embedder import Model\n",
    "from evaluator import Evaluator\n",
    "from visualizator import Visualizator\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "dim = np.logspace(1, 5, num=5, base=2)\n",
    "vis = Visualizator(dim, n_classifiers = 2)\n",
    "for d in dim:\n",
    "    # dimension of the last embedding\n",
    "    dimension_embedding = int(d)\n",
    "\n",
    "\n",
    "\n",
    "    # instanziate second model\n",
    "    pca = TruncatedSVD(n_components=dimension_embedding)\n",
    "    res = pca.fit_transform(predictions)\n",
    "\n",
    "    evaluator = Evaluator(KNeighborsClassifier(n_neighbors=1))\n",
    "\n",
    "    # compute the metrics\n",
    "    acc, pre, rec, f = evaluator.performance_with_kfold(res,y_test_tmp)\n",
    "\n",
    "    # add the computed performance in the visualizzator\n",
    "    vis.add_metrics(acc,pre,rec,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions:  [ 2.  4.  8. 16. 32.]\n",
      "\n",
      "Train set size:  169\n",
      "Test set size:  19\n",
      "*****************************\n",
      "knn\n",
      "                2.0       4.0       8.0      16.0      32.0\n",
      "---------  --------  --------  --------  --------  --------\n",
      "Accuracy   0.766667  0.766667  0.766667  0.766667  0.766667\n",
      "Precision  0.8       0.8       0.8       0.8       0.8\n",
      "Recall     0.85      0.85      0.85      0.85      0.85\n",
      "F1         0.8       0.8       0.8       0.8       0.8\n"
     ]
    }
   ],
   "source": [
    "vis.summarize(len(x_train),len(x_test),method_name_1=\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**********************************************************\n",
      "Model:  1\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis.single_plot(y_test_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4716518e-01, 3.5283479e-01],\n",
       "       [2.4311149e-01, 7.5688857e-01],\n",
       "       [2.1634139e-03, 9.9783665e-01],\n",
       "       [7.3817295e-01, 2.6182705e-01],\n",
       "       [3.1925946e-02, 9.6807408e-01],\n",
       "       [3.1925932e-02, 9.6807408e-01],\n",
       "       [9.0586960e-01, 9.4130427e-02],\n",
       "       [3.3406022e-01, 6.6593981e-01],\n",
       "       [3.3406016e-01, 6.6593981e-01],\n",
       "       [4.0850177e-02, 9.5914978e-01],\n",
       "       [2.6057580e-02, 9.7394240e-01],\n",
       "       [3.3406022e-01, 6.6593981e-01],\n",
       "       [3.5600344e-04, 9.9964404e-01],\n",
       "       [1.7161557e-02, 9.8283845e-01],\n",
       "       [3.2080173e-01, 6.7919827e-01],\n",
       "       [4.1326538e-01, 5.8673471e-01],\n",
       "       [3.5396597e-01, 6.4603406e-01],\n",
       "       [2.1634151e-03, 9.9783665e-01],\n",
       "       [3.6029441e-03, 9.9639708e-01]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = np.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 28, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 28, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_2 (GraphConv)        (None, 28, 32)       128         input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_2 (GraphAttenti (None, 28, 32)       1120        graph_conv_2[0][0]               \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_3 (GraphAttenti (None, 28, 32)       1120        graph_attention_2[0][0]          \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_attention_pool_5 (Global (None, 128)          8448        graph_attention_3[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 10,816\n",
      "Trainable params: 10,816\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pool = GlobalAttentionPool(128)(gc3)\n",
    "\n",
    "# Build model\n",
    "model_1= Model(inputs=[X_in, filter_in], outputs=pool)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_1.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.25      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.25      ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.25      , 0.5       , 0.2236068 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.2236068 , 0.4       , 0.2236068 , 0.        ,\n",
       "        0.        , 0.        , 0.2       , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.2236068 , 0.5       , 0.25      ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.25      , 0.5       ,\n",
       "        0.25      , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.25      ,\n",
       "        0.5       , 0.25      , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25      , 0.5       , 0.2236068 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.2       , 0.        , 0.        ,\n",
       "        0.        , 0.2236068 , 0.4       , 0.2       , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.2       , 0.4       , 0.2236068 ,\n",
       "        0.2       , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.25      , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.2236068 , 0.5       ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.2       , 0.        ,\n",
       "        0.4       , 0.25819889, 0.25819889, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25819889, 0.66666667, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25819889, 0.        , 0.66666667, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 1.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 1.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model_1.predict([x_test[0:1], fltr_test[0:1]],batch_size=batch_size)\n",
    "len(res[0])\n",
    "\n",
    "fltr_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "768/768 [==============================] - 2s 3ms/step - loss: 2.8885 - acc: 0.6406\n",
      "Epoch 2/10\n",
      "768/768 [==============================] - 0s 395us/step - loss: 1.2661 - acc: 0.5742\n",
      "Epoch 3/10\n",
      "768/768 [==============================] - 0s 432us/step - loss: 0.7106 - acc: 0.5951\n",
      "Epoch 4/10\n",
      "768/768 [==============================] - 0s 426us/step - loss: 0.6626 - acc: 0.6354\n",
      "Epoch 5/10\n",
      "768/768 [==============================] - 0s 413us/step - loss: 0.6410 - acc: 0.6445\n",
      "Epoch 6/10\n",
      "768/768 [==============================] - 0s 400us/step - loss: 0.6417 - acc: 0.6484\n",
      "Epoch 7/10\n",
      "768/768 [==============================] - 0s 444us/step - loss: 0.6446 - acc: 0.6276\n",
      "Epoch 8/10\n",
      "768/768 [==============================] - 0s 457us/step - loss: 0.6196 - acc: 0.6615\n",
      "Epoch 9/10\n",
      "768/768 [==============================] - 0s 466us/step - loss: 0.6257 - acc: 0.6549\n",
      "Epoch 10/10\n",
      "768/768 [==============================] - 0s 462us/step - loss: 0.6162 - acc: 0.6784\n",
      "768/768 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5929284024362763, 0.6835937520954758]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "l1 = layers.Dense(12, input_dim=8, activation='relu')\n",
    "l2 = layers.Dense(8, activation='relu')\n",
    "l3 = layers.Dense(1, activation='sigmoid')\n",
    "model.add(l1)\n",
    "model.add(l2)\n",
    "model.add(l3)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, epochs=10, batch_size=10)\n",
    "\n",
    "model.evaluate(X,Y,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.5929284024362763, 0.6835937520954758]\n",
    "[0.5946809300221503, 0.6770833358168602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(l1)\n",
    "model_2.add(l2)\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "rest = model_2.predict(X,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rest[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "call() got an unexpected keyword argument 'include_top'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-f0490a90b1b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minpute_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minclude_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minpute_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tesi/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Actually call the layer,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: call() got an unexpected keyword argument 'include_top'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "'''\n",
    "##Input(shape=(N, F)) \n",
    "w = model.get_weights()\n",
    "inpute_test = Input(shape=(8,8))\n",
    "base_model = model( include_top = False, inputs = inpute_test)\n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(base_model)\n",
    "new_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "new_model.summary()\n",
    "#plot_model(new_model, to_file='new_model_Ese_1.png', show_shapes=True)\n",
    "\n",
    "# Fit the model\n",
    "# Fit the model\n",
    "new_model.predict(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.      , 34.803635, -0.      , -0.      , 33.3918  , 42.032394,\n",
       "        -0.      , 11.876795]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,4]\n",
    "b = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "YY = [[Y[i],Y[i]] for i in range(0,len(Y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "768/768 [==============================] - 2s 2ms/step - loss: 0.6834 - acc: 0.6510\n",
      "Epoch 2/10\n",
      "768/768 [==============================] - 1s 654us/step - loss: 0.6730 - acc: 0.6510\n",
      "Epoch 3/10\n",
      "768/768 [==============================] - 0s 636us/step - loss: 0.6679 - acc: 0.6510\n",
      "Epoch 4/10\n",
      "768/768 [==============================] - 1s 653us/step - loss: 0.6636 - acc: 0.6510\n",
      "Epoch 5/10\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.6606 - acc: 0.6510\n",
      "Epoch 6/10\n",
      "768/768 [==============================] - 0s 463us/step - loss: 0.6575 - acc: 0.6510\n",
      "Epoch 7/10\n",
      "768/768 [==============================] - 0s 460us/step - loss: 0.6566 - acc: 0.6510\n",
      "Epoch 8/10\n",
      "768/768 [==============================] - 0s 437us/step - loss: 0.6536 - acc: 0.6510\n",
      "Epoch 9/10\n",
      "768/768 [==============================] - 0s 453us/step - loss: 0.6522 - acc: 0.6510\n",
      "Epoch 10/10\n",
      "768/768 [==============================] - 0s 419us/step - loss: 0.6502 - acc: 0.6510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f487ff77f60>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, Y, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 409us/step\n",
      "\n",
      "acc: 65.10%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/anaconda3/envs/tesi/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x7f488c191828>,\n",
       " <keras.layers.core.Dense at 0x7f488c191438>,\n",
       " <keras.layers.core.Dense at 0x7f488c1910b8>,\n",
       " <keras.layers.core.Dense at 0x7f4885206518>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35438338],\n",
       "       [0.36234334],\n",
       "       [0.37364897],\n",
       "       [0.4048949 ]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheby : K = 10\n",
    "#ARMA k = [3,4] shred W no\n",
    "# p drop = 0.25\n",
    "\n",
    "# Parameters\n",
    "N = x.shape[-2]           # Number of nodes in the graphs\n",
    "F = x.shape[-1]           # Original feature dimensionality\n",
    "n_classes = y.shape[-1]   # Number of classes\n",
    "l2_reg = 5e-4             # Regularization rate for l2\n",
    "learning_rate = 1e-3      # Learning rate for Adam\n",
    "epochs = 200            # Number of training epochs\n",
    "batch_size = 32           # Batch size\n",
    "es_patience = 250          # Patience fot early stopping\n",
    "log_dir = init_logging()  # Create log directory and file\n",
    "\n",
    "# Preprocessing\n",
    "fltr = localpooling_filter(adj.copy())\n",
    "\n",
    "# Train/test split\n",
    "fltr_train, fltr_test, \\\n",
    "x_train, x_test,       \\\n",
    "y_train, y_test = train_test_split(fltr, x, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 28, 3)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 28, 28)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_3 (GraphConv)        (None, 28, 32)       128         input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 28, 2)        66          graph_conv_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 194\n",
      "Trainable params: 194\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "X_in = Input(shape=(N, F)) # shape 28 * 3\n",
    "filter_in = Input((N, N))  # shape 28 * 28\n",
    "\n",
    "# input  [(28*3),(28x28)]\n",
    "# output [28x32]\n",
    "gc1 = GraphConv(32, activation='relu', kernel_regularizer=l2(l2_reg))([X_in, filter_in]) \n",
    "\n",
    "\n",
    "output = Dense(n_classes, activation='softmax')(gc1)\n",
    "\n",
    "# Build model\n",
    "model_1= Model(inputs=[X_in, filter_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "model_1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have 3 dimensions, but got array with shape (169, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-ce8746816f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           callbacks=[es_callback])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tesi/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tesi/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tesi/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have 3 dimensions, but got array with shape (169, 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_acc', patience=es_patience)\n",
    "\n",
    "# Train model\n",
    "model_1.fit([x_train, fltr_train],\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_split=0.1,\n",
    "          epochs=epochs,\n",
    "          callbacks=[es_callback])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-4e55d21a23c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cora'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcitation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tesi/lib/python3.6/site-packages/spektral/datasets/citation.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0midx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_idx_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0midx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0midx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sample_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_size' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from spektral.datasets import citation\n",
    "from spektral.layers import ARMAConv\n",
    "from spektral.utils import normalized_laplacian, rescale_laplacian\n",
    "from spektral.utils.logging import init_logging\n",
    "\n",
    "# Load data\n",
    "dataset = 'cora'\n",
    "adj, node_features, y_train, y_val, y_test, train_mask, val_mask, test_mask = citation.load_data(dataset)\n",
    "\n",
    "# Parameters\n",
    "ARMA_K = 2                    # Number of parallel ARMA_1 filters\n",
    "ARMA_D = 1                    # Depth of each ARMA_1 filter\n",
    "recurrent = True              # Share weights like a recurrent net in each head\n",
    "N = node_features.shape[0]    # Number of nodes in the graph\n",
    "F = node_features.shape[1]    # Original feature dimensionality\n",
    "n_classes = y_train.shape[1]  # Number of classes\n",
    "dropout_rate = 0.75           # Dropout rate applied to the input of GCN layers\n",
    "l2_reg = 5e-4                 # Regularization rate for l2\n",
    "learning_rate = 1e-2          # Learning rate for SGD\n",
    "epochs = 20000                # Number of training epochs\n",
    "es_patience = 200             # Patience for early stopping\n",
    "log_dir = init_logging()      # Create log directory and file\n",
    "\n",
    "# Preprocessing operations\n",
    "node_features = citation.preprocess_features(node_features)\n",
    "fltr = normalized_laplacian(adj, symmetric=True)\n",
    "fltr = rescale_laplacian(fltr, lmax=2)\n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(F, ))\n",
    "fltr_in = Input((N, ), sparse=True)\n",
    "\n",
    "dropout_1 = Dropout(dropout_rate)(X_in)\n",
    "graph_conv_1 = ARMAConv(16,\n",
    "                        ARMA_K=ARMA_K,\n",
    "                        ARMA_D=ARMA_D,\n",
    "                        recurrent=recurrent,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        activation='elu',\n",
    "                        gcn_activation='elu',\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        use_bias=True)([dropout_1, fltr_in])\n",
    "dropout_2 = Dropout(dropout_rate)(graph_conv_1)\n",
    "graph_conv_2 = ARMAConv(n_classes,\n",
    "                        ARMA_K=1,\n",
    "                        ARMA_D=1,\n",
    "                        recurrent=recurrent,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        activation='softmax',\n",
    "                        gcn_activation=None,\n",
    "                        kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        use_bias=True)([dropout_2, fltr_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, fltr_in], outputs=graph_conv_2)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "tb_callback = TensorBoard(log_dir=log_dir, batch_size=N, write_graph=True)\n",
    "mc_callback = ModelCheckpoint(log_dir + 'best_model.h5',\n",
    "                              monitor='val_weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "\n",
    "# Train model\n",
    "validation_data = ([node_features, fltr], y_val, val_mask)\n",
    "model.fit([node_features, fltr],\n",
    "          y_train,\n",
    "          sample_weight=train_mask,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback, tb_callback, mc_callback])\n",
    "\n",
    "# Load best model\n",
    "model.load_weights(log_dir + 'best_model.h5')\n",
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model.evaluate([node_features, fltr],\n",
    "                              y_test,\n",
    "                              sample_weight=test_mask,\n",
    "                              batch_size=N)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
