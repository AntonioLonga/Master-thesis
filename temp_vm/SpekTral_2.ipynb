{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example implements the experiments on citation networks from the paper:\n",
    "Graph Neural Networks with convolutional ARMA filters (https://arxiv.org/abs/1901.01343)\n",
    "Filippo Maria Bianchi, Daniele Grattarola, Cesare Alippi, Lorenzo Livi\n",
    "\"\"\"\n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from spektral.datasets import citation\n",
    "from spektral.layers import ARMAConv\n",
    "from spektral.utils import normalized_laplacian, rescale_laplacian\n",
    "from spektral.utils.logging import init_logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "dataset = 'cora'\n",
    "adj, node_features, y_train, y_val, y_test, train_mask, val_mask, test_mask = citation.load_data(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "ARMA_K = 3                    # Number of parallel ARMA_1 filters\n",
    "ARMA_D = 2                    # Depth of each ARMA_1 filter\n",
    "recurrent = True              # Share weights like a recurrent net in each head\n",
    "N = node_features.shape[0]    # Number of nodes in the graph\n",
    "F = node_features.shape[1]    # Original feature dimensionality\n",
    "n_classes = y_train.shape[1]  # Number of classes\n",
    "dropout_rate = 0.25           # Dropout rate applied to the input of GCN layers\n",
    "l2_reg = 5e-4                 # Regularization rate for l2\n",
    "learning_rate = 1e-2          # Learning rate for SGD\n",
    "epochs = 20               # Number of training epochs\n",
    "es_patience = 200             # Patience for early stopping\n",
    "log_dir = init_logging()      # Create log directory and file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2708x1433 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 49216 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 1433)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 1433)         0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 2708)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arma_conv_5 (ARMAConv)          (None, 16)           207216      dropout_15[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16)           0           arma_conv_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "arma_conv_6 (ARMAConv)          (None, 7)            231         dropout_22[0][0]                 \n",
      "                                                                 input_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 207,447\n",
      "Trainable params: 207,447\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing operations\n",
    "node_features = citation.preprocess_features(node_features)\n",
    "fltr = normalized_laplacian(adj, symmetric=True)\n",
    "fltr = rescale_laplacian(fltr, lmax=2)\n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(F, ), name=\"input\")\n",
    "fltr_in = Input((N, ), sparse=True)\n",
    "\n",
    "dropout_1 = Dropout(dropout_rate)(X_in)\n",
    "graph_conv_1 = ARMAConv(16,\n",
    "                        ARMA_K=ARMA_K,\n",
    "                        ARMA_D=ARMA_D,\n",
    "                        recurrent=recurrent,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        activation='elu',\n",
    "                        gcn_activation='elu',\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        use_bias=True)([dropout_1, fltr_in])\n",
    "dropout_2 = Dropout(dropout_rate)(graph_conv_1)\n",
    "graph_conv_2 = ARMAConv(n_classes,\n",
    "                        ARMA_K=1,\n",
    "                        ARMA_D=1,\n",
    "                        recurrent=recurrent,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        activation='softmax',\n",
    "                        gcn_activation=None,\n",
    "                        kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(l2_reg),\n",
    "                        use_bias=True)([dropout_2, fltr_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, fltr_in], outputs=graph_conv_2)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file=\"semi-super-node-CORA.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2708 samples, validate on 2708 samples\n",
      "Epoch 1/20\n",
      "2708/2708 [==============================] - 3s 1ms/step - loss: 2.1253 - weighted_acc: 0.0929 - val_loss: 2.0777 - val_weighted_acc: 0.1520\n",
      "Epoch 2/20\n",
      "2708/2708 [==============================] - 0s 181us/step - loss: 2.0429 - weighted_acc: 0.3929 - val_loss: 2.0372 - val_weighted_acc: 0.2700\n",
      "Epoch 3/20\n",
      "2708/2708 [==============================] - 1s 191us/step - loss: 1.9795 - weighted_acc: 0.5857 - val_loss: 2.0077 - val_weighted_acc: 0.3540\n",
      "Epoch 4/20\n",
      "2708/2708 [==============================] - 1s 199us/step - loss: 1.9239 - weighted_acc: 0.5857 - val_loss: 1.9826 - val_weighted_acc: 0.4740\n",
      "Epoch 5/20\n",
      "2708/2708 [==============================] - 1s 189us/step - loss: 1.8726 - weighted_acc: 0.7000 - val_loss: 1.9562 - val_weighted_acc: 0.6600\n",
      "Epoch 6/20\n",
      "2708/2708 [==============================] - 1s 192us/step - loss: 1.8249 - weighted_acc: 0.8643 - val_loss: 1.9332 - val_weighted_acc: 0.7080\n",
      "Epoch 7/20\n",
      "2708/2708 [==============================] - 1s 192us/step - loss: 1.7779 - weighted_acc: 0.9000 - val_loss: 1.9131 - val_weighted_acc: 0.7220\n",
      "Epoch 8/20\n",
      "2708/2708 [==============================] - 1s 197us/step - loss: 1.7326 - weighted_acc: 0.8643 - val_loss: 1.8973 - val_weighted_acc: 0.7280\n",
      "Epoch 9/20\n",
      "2708/2708 [==============================] - 1s 190us/step - loss: 1.6780 - weighted_acc: 0.9357 - val_loss: 1.8830 - val_weighted_acc: 0.7420\n",
      "Epoch 10/20\n",
      "2708/2708 [==============================] - 1s 189us/step - loss: 1.6473 - weighted_acc: 0.8571 - val_loss: 1.8702 - val_weighted_acc: 0.7560\n",
      "Epoch 11/20\n",
      "2708/2708 [==============================] - 1s 187us/step - loss: 1.6046 - weighted_acc: 0.9286 - val_loss: 1.8603 - val_weighted_acc: 0.7540\n",
      "Epoch 12/20\n",
      "2708/2708 [==============================] - 1s 199us/step - loss: 1.5674 - weighted_acc: 0.9286 - val_loss: 1.8518 - val_weighted_acc: 0.7200\n",
      "Epoch 13/20\n",
      "2708/2708 [==============================] - 1s 295us/step - loss: 1.5000 - weighted_acc: 0.9286 - val_loss: 1.8384 - val_weighted_acc: 0.6900\n",
      "Epoch 14/20\n",
      "2708/2708 [==============================] - 1s 281us/step - loss: 1.4734 - weighted_acc: 0.9500 - val_loss: 1.8211 - val_weighted_acc: 0.6860\n",
      "Epoch 15/20\n",
      "2708/2708 [==============================] - 1s 219us/step - loss: 1.4258 - weighted_acc: 0.9143 - val_loss: 1.7968 - val_weighted_acc: 0.6880\n",
      "Epoch 16/20\n",
      "2708/2708 [==============================] - 1s 261us/step - loss: 1.4042 - weighted_acc: 0.9286 - val_loss: 1.7643 - val_weighted_acc: 0.7240\n",
      "Epoch 17/20\n",
      "2708/2708 [==============================] - 1s 218us/step - loss: 1.3306 - weighted_acc: 0.9500 - val_loss: 1.7250 - val_weighted_acc: 0.7540\n",
      "Epoch 18/20\n",
      "2708/2708 [==============================] - 1s 198us/step - loss: 1.2431 - weighted_acc: 0.9357 - val_loss: 1.6870 - val_weighted_acc: 0.7660\n",
      "Epoch 19/20\n",
      "2708/2708 [==============================] - 1s 215us/step - loss: 1.2334 - weighted_acc: 0.9500 - val_loss: 1.6542 - val_weighted_acc: 0.7840\n",
      "Epoch 20/20\n",
      "2708/2708 [==============================] - 1s 354us/step - loss: 1.1659 - weighted_acc: 0.9500 - val_loss: 1.6300 - val_weighted_acc: 0.7760\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "tb_callback = TensorBoard(log_dir=log_dir, batch_size=N, write_graph=True)\n",
    "mc_callback = ModelCheckpoint(log_dir + 'best_model.h5',\n",
    "                              monitor='val_weighted_acc',\n",
    "                              save_best_only=True,\n",
    "                              save_weights_only=True)\n",
    "\n",
    "# Train model\n",
    "validation_data = ([node_features, fltr], y_val, val_mask)\n",
    "model.fit([node_features, fltr],\n",
    "          y_train,\n",
    "          sample_weight=train_mask,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback, tb_callback, mc_callback])\n",
    "\n",
    "# Load best model\n",
    "model.load_weights(log_dir + 'best_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model.\n",
      "2708/2708 [==============================] - 0s 70us/step\n",
      "Done.\n",
      "Test loss: 1.641263723373413\n",
      "Test accuracy: 0.7870006561279297\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model.evaluate([node_features, fltr],\n",
    "                              y_test,\n",
    "                              sample_weight=test_mask,\n",
    "                              batch_size=N)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from spektral.datasets import mnist\n",
    "from spektral.layers import GraphConv\n",
    "from spektral.utils import init_logging, normalized_laplacian\n",
    "\n",
    "\n",
    "def sp_matrix_to_sp_tensor(x):\n",
    "    x = x.tocoo()\n",
    "    return tf.SparseTensor(indices=np.array([x.row, x.col]).T,\n",
    "                           values=x.data, dense_shape=x.shape)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "l2_reg = 5e-4             # Regularization rate for l2\n",
    "learning_rate = 1e-3      # Learning rate for SGD\n",
    "batch_size = 100           # Batch size\n",
    "epochs = 1            # Number of training epochs\n",
    "es_patience = 200         # Patience fot early stopping\n",
    "log_dir = init_logging()  # Create log directory and file\n",
    "\n",
    "# Load data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, adj = mnist.load_data()\n",
    "\n",
    "X_train, X_val, X_test = X_train[..., None], X_val[..., None], X_test[..., None]\n",
    "N = X_train.shape[-2]      # Number of nodes in the graphs\n",
    "F = X_train.shape[-1]      # Node features dimensionality\n",
    "n_out = y_train.shape[-1]  # Dimension of the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           (None, 784, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (784, 784)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_5 (GraphConv)        (None, 784, 32)      64          input_10[0][0]                   \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_6 (GraphConv)        (None, 784, 32)      1056        graph_conv_5[0][0]               \n",
      "                                                                 input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 25088)        0           graph_conv_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          12845568    flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 50000)        25650000    dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,496,688\n",
      "Trainable params: 38,496,688\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fltr = normalized_laplacian(adj)\n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(N, F))\n",
    "# Pass filter as a fixed tensor, otherwise Keras will complain about inputs of\n",
    "# different rank.\n",
    "G_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
    "\n",
    "graph_conv = GraphConv(32,\n",
    "                       activation='elu',\n",
    "                       kernel_regularizer=l2(l2_reg),\n",
    "                       use_bias=True)([X_in, G_in])\n",
    "\n",
    "graph_conv = GraphConv(32,\n",
    "                       activation='elu',\n",
    "                       kernel_regularizer=l2(l2_reg),\n",
    "                       use_bias=True)([graph_conv, G_in])\n",
    "flatten = Flatten()(graph_conv)\n",
    "fc = Dense(512, activation='relu')(flatten)\n",
    "output = Dense(n_out, activation='softmax')(fc)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, G_in], outputs=output)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 747s 15ms/step - loss: 1.0370 - acc: 0.7280 - val_loss: 0.4379 - val_acc: 0.8749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff172268390>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "validation_data = (X_val, y_val)\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=validation_data,\n",
    "          epochs=epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model.\n",
      "10000/10000 [==============================] - 56s 6ms/step\n",
      "Done.\n",
      "Test loss: 0.41450210615992544\n",
      "Test acc: 0.8805000030994415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model.evaluate(X_test,\n",
    "                              y_test,\n",
    "                              batch_size=batch_size)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "'Test acc: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
